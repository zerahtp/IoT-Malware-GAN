import os
import logging
import numpy as np
import tensorflow as tf
from sklearn.impute import KNNImputer
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

#bu sayede sadece hata mesajları görüntülenir ve diğer günlük mesajları gösterilmez.
tf.get_logger().setLevel(logging.ERROR)

#Veri seti okuma
data = pd.read_csv('C:/Users/dell/Desktop/resources/veri.csv', encoding='latin-1')

#giriş olarak aldığı bir veri çerçevesini alır ve sütun isimlerini sıralı sayılara dönüştürerek döndürür.
def _df(data):
    df = pd.DataFrame(data)
    #DataFrame'deki sütun sayısı kadar bir döngü oluşturur.
    for c in range(df.shape[1]):
        #her bir sütun adını ve ona karşılık gelen bir sayıyı içeren bir sözlük oluşturur.
        mapping = {df.columns[c]: c}
        #DataFrame'in sütunlarını oluşturulan sözlükle yeniden adlandırır, yani sütun isimleri sıralı sayılara dönüştürülür.
        df = df.rename(columns=mapping)
    return df

#Verileri giris ve cikis olarak ayiriyoruz...
#"Malware_Durumu" adlı sütunu çıkararak X değişkenine giriş verilerini atar.
X = (data.drop(columns=["Malware_Durumu"])).values
y = (data["Malware_Durumu"]).values


#K-Nearest Neighbors (KNN) algoritması kullanılarak eksik değerleri doldurur. fit_transform metodu, modeli eğitip dönüştürmeyi sağlar.
X = KNNImputer().fit_transform(X)
#giriş verilerini (X) ve çıkış verilerini (y) birleştirir, StandardScaler kullanarak ölçekler,
data = _df(StandardScaler().fit_transform(np.column_stack((X, y))))

import os
import logging
import numpy as np
import pandas as pd
import tensorflow as tf

tf.get_logger().setLevel(logging.ERROR)


class Gan():

    def __init__(self, data):
        #sınıfın constructor tanımlar

        self.data = data
        self.n_epochs = 200

    # Random noise uret...
    def _noise(self):
        noise = np.random.normal(0, 1, self.data.shape)
        return noise

    def _generator(self):
        model = tf.keras.Sequential(name="Generator_model")
        model.add(tf.keras.layers.Dense(15, activation='relu',
                                        kernel_initializer='he_uniform',
                                        input_dim=self.data.shape[1]))
        model.add(tf.keras.layers.Dense(30, activation='relu'))
        model.add(tf.keras.layers.Dense(
            self.data.shape[1], activation='linear'))
        return model

    def _discriminator(self):
        model = tf.keras.Sequential(name="Discriminator_model")
        model.add(tf.keras.layers.Dense(25, activation='relu',
                                        kernel_initializer='he_uniform',
                                        input_dim=self.data.shape[1]))
        model.add(tf.keras.layers.Dense(50, activation='relu'))
        # sigmoid => Cunku gercek / fake ayrimi icin...
        model.add(tf.keras.layers.Dense(1, activation='sigmoid'))
        model.compile(loss='binary_crossentropy',
                      optimizer='adam',
                      metrics=['accuracy'])

        return model

    def _GAN(self, generator, discriminator):
        #discriminator'un eğitilebilir (trainable) olmasını engeller. GAN eğitimi sırasında, sadece üreteç üzerinde güncelleme yapılırken discriminator güncellenmez. Bu sayede, üreteç daha iyi sahte veri üretmeyi öğrenir.
        discriminator.trainable = False
        generator.trainable = True
        model = tf.keras.Sequential(name="GAN")
        model.add(generator)
        model.add(discriminator)
        model.compile(loss='binary_crossentropy', optimizer='adam')
        return model

    # Generator ve Discriminator Egitimi
    def train(self, generator, discriminator, gan):

        for epoch in range(self.n_epochs):
            
            # Disc. modeli egit
            generated_data = generator.predict(self._noise())
            #Bu satır, etiketleri oluşturur. Gerçek veri için etiketler "1", sahte veri için etiketler "0" olacak şekilde iki seti birleştirir.
            labels = np.concatenate([np.ones(self.data.shape[0]), np.zeros(self.data.shape[0])])
            #Bu satır, gerçek veri seti ile üretilen sahte veri setini birleştirerek toplam veri setini oluşturur.
            X = np.concatenate([self.data, generated_data])
            discriminator.trainable = True
            d_loss , _ = discriminator.train_on_batch(X, labels)

            # Egitim
            noise = self._noise()
            g_loss = gan.train_on_batch(noise, np.ones(self.data.shape[0]))


            print('>%d, d1=%.3f, d2=%.3f' %(epoch+1, d_loss, g_loss))

        return generator

#GAN model olusumu
model = Gan(data=data)
generator = model._generator()
descriminator = model._discriminator()
gan_model = model._GAN(generator=generator, discriminator=descriminator)
trained_model = model.train(
    generator=generator, discriminator=descriminator, gan=gan_model)

#Gurultu ile baslangic
noise = np.random.normal(0, 1, data.shape) 
#Sentetik veri olusur.
new_data = _df(data=trained_model.predict(noise))
new_data.to_excel("C:/Users/dell/Desktop/resources/sentetik_veri.xlsx")

#Gorseller
#Confusion Matrix
fig, ax = plt.subplots(1, 2, figsize=(20, 6))
sns.heatmap(data.corr(), annot=True, ax=ax[0], cmap="Reds")
sns.heatmap(new_data.corr(), annot=True, ax=ax[1], cmap="Reds")
ax[0].set_title("Orijinal Veri")
ax[1].set_title("Yeni (Sentetik) Veri")

#Veri Dagilimi
fig, ax = plt.subplots(1, 2, figsize=(20, 6))
ax[0].scatter(data.iloc[:, 0], data.iloc[:, 1])
ax[1].scatter(new_data.iloc[:, 0], new_data.iloc[:, 1])
ax[0].set_title("Orijinal Veri")
ax[1].set_title("Yeni (Sentetik) Veri")
